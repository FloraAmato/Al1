# CREA OSS Platform

**Open-Source Chatbot + RAG + Game-Theory Dispute Resolution Platform**

A modern, production-ready platform combining conversational AI with fair division algorithms for dispute resolution. Built entirely on open-source technologies.

## ğŸŒŸ Features

### Core Capabilities

- **ğŸ’¬ Conversational AI**
  - Multi-backend LLM support (HuggingFace, TGI, vLLM, llama.cpp)
  - Streaming responses
  - Context-aware conversations with Redis-backed history
  - Fine-tuning data collection and export

- **ğŸ“š RAG (Retrieval-Augmented Generation)**
  - Document ingestion and chunking
  - Open-source embeddings (sentence-transformers)
  - Qdrant vector database integration
  - Semantic search with configurable relevance thresholds

- **âš–ï¸ Game-Theory Algorithms**
  - **Max-Min Fairness**: Maximize minimum agent utility
  - **Nash Social Welfare**: Maximize product of weighted utilities
  - Support for entitlements, restrictions, and budgets
  - Real-time dispute resolution with explanations

- **âš¡ Performance & Scalability**
  - Semantic caching (Redis)
  - Background task processing (Celery)
  - Async database operations (SQLAlchemy + asyncpg)
  - Docker-based deployment

- **ğŸ¯ Model Management**
  - Swap between base and fine-tuned models
  - Multi-tenant model configurations
  - Support for local and remote inference

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        FastAPI Backend                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   ChatService   â”‚  RAGService  â”‚ LLMService  â”‚ OptimizerSvc â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Redis   â”‚  â”‚  Qdrant    â”‚  â”‚   LLM    â”‚  â”‚Algorithm  â”‚ â”‚
â”‚  â”‚  Cache   â”‚  â”‚  Vector DB â”‚  â”‚ Backends â”‚  â”‚  Solvers  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚    PostgreSQL Database    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Components

1. **Backend** (FastAPI)
   - RESTful API endpoints
   - Async request handling
   - Pydantic validation
   - Structured logging

2. **LLM Layer**
   - Modular backend abstraction
   - Support for multiple open-source models
   - Configurable generation parameters

3. **RAG Pipeline**
   - Document chunking and embedding
   - Vector similarity search
   - Context injection into prompts

4. **Game-Theory Solvers**
   - SciPy-based optimization
   - Constraint handling
   - Fair allocation computation

5. **Caching & Workers**
   - Semantic prompt caching
   - Background document processing
   - Training data export

## ğŸš€ Quick Start

### Prerequisites

- Docker & Docker Compose
- Python 3.11+ (for local development)
- 8GB+ RAM recommended

### Installation

1. **Clone the repository**

```bash
git clone <repository-url>
cd crea-oss-platform
```

2. **Configure environment**

```bash
cd backend
cp .env.example .env
# Edit .env with your configuration
```

3. **Start with Docker Compose**

```bash
cd ../infra
docker-compose up -d
```

This will start:
- PostgreSQL (port 5432)
- Redis (port 6379)
- Qdrant (port 6333)
- FastAPI backend (port 8000)
- Celery worker

4. **Initialize database** (first time only)

```bash
docker-compose exec backend alembic upgrade head
```

5. **Access the API**

- API: http://localhost:8000
- API Docs: http://localhost:8000/docs
- Qdrant UI: http://localhost:6333/dashboard

## ğŸ“– Usage

### Chat API

Send a message to the chatbot:

```bash
curl -X POST "http://localhost:8000/api/v1/chat/" \
  -H "Content-Type: application/json" \
  -d '{
    "message": "What is max-min fairness allocation?",
    "session_id": "user-123",
    "use_rag": true
  }'
```

### Create a Dispute

```bash
curl -X POST "http://localhost:8000/api/v1/disputes/" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Inheritance Dispute",
    "resolution_method": "ratings",
    "agents": [
      {"email": "alice@example.com", "name": "Alice", "share_of_entitlement": 0.5},
      {"email": "bob@example.com", "name": "Bob", "share_of_entitlement": 0.5}
    ],
    "goods": [
      {"name": "House", "estimated_value": 300000, "indivisible": true},
      {"name": "Savings", "estimated_value": 50000, "indivisible": false}
    ]
  }'
```

### Solve a Dispute

```bash
curl -X POST "http://localhost:8000/api/v1/disputes/1/solve" \
  -H "Content-Type: application/json" \
  -d '{"method": "maxmin"}'
```

### Upload Documents for RAG

```bash
curl -X POST "http://localhost:8000/api/v1/documents/upload" \
  -F "file=@legal_document.txt" \
  -F "title=Legal Precedent Document" \
  -F "category=laws"
```

## ğŸ“ Model Configuration

### Using Different LLM Backends

#### 1. HuggingFace Local (Default)

```python
# In .env
DEFAULT_LLM_BACKEND=hf_local
DEFAULT_MODEL_ID=mistralai/Mistral-7B-Instruct-v0.2
HF_DEVICE=cuda  # or cpu
```

#### 2. Text Generation Inference (TGI)

```bash
# Start TGI server
docker run -p 8080:80 \
  -v $PWD/models:/data \
  ghcr.io/huggingface/text-generation-inference:latest \
  --model-id mistralai/Mistral-7B-Instruct-v0.2
```

```python
# In .env
DEFAULT_LLM_BACKEND=tgi
TGI_URL=http://localhost:8080
```

#### 3. vLLM

```bash
# Start vLLM server
python -m vllm.entrypoints.openai.api_server \
  --model mistralai/Mistral-7B-Instruct-v0.2 \
  --port 8000
```

```python
# In .env
DEFAULT_LLM_BACKEND=vllm
VLLM_URL=http://localhost:8000
```

### Fine-Tuning Integration

1. **Collect training data**

```bash
# Export conversations with rating >= 4
curl -X POST "http://localhost:8000/api/v1/admin/export-training-data?min_rating=4"
```

2. **Train your model** (external process)

Use the exported JSONL with your preferred fine-tuning framework (e.g., LoRA, QLoRA).

3. **Register the fine-tuned model**

```bash
curl -X POST "http://localhost:8000/api/v1/models/" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "dispute-finetuned-v1",
    "model_id": "path/to/finetuned-model",
    "backend_type": "hf_local",
    "is_finetuned": true,
    "base_model_id": "mistralai/Mistral-7B-Instruct-v0.2"
  }'
```

4. **Use the fine-tuned model**

```bash
curl -X POST "http://localhost:8000/api/v1/chat/" \
  -d '{
    "message": "Analyze this dispute...",
    "session_id": "user-123",
    "model_id": "path/to/finetuned-model"
  }'
```

## ğŸ§ª Testing

```bash
cd backend

# Install dev dependencies
pip install -r requirements.txt

# Run tests
pytest

# With coverage
pytest --cov=app --cov-report=html
```

## ğŸ“ Project Structure

```
crea-oss-platform/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ algorithms/      # Game-theory solvers
â”‚   â”‚   â”œâ”€â”€ api/             # FastAPI routes
â”‚   â”‚   â”œâ”€â”€ core/            # Config, logging, database
â”‚   â”‚   â”œâ”€â”€ llm_backends/    # LLM backend implementations
â”‚   â”‚   â”œâ”€â”€ models/          # SQLAlchemy models
â”‚   â”‚   â”œâ”€â”€ rag/             # RAG components
â”‚   â”‚   â”œâ”€â”€ schemas/         # Pydantic schemas
â”‚   â”‚   â”œâ”€â”€ services/        # Business logic
â”‚   â”‚   â”œâ”€â”€ workers/         # Celery tasks
â”‚   â”‚   â””â”€â”€ main.py          # FastAPI app
â”‚   â”œâ”€â”€ tests/               # Tests
â”‚   â”œâ”€â”€ alembic/             # Database migrations
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ .env.example
â”œâ”€â”€ infra/
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â””â”€â”€ Dockerfile.backend
â”œâ”€â”€ docs/                    # Additional documentation
â””â”€â”€ README.md
```

## ğŸ”§ Configuration Reference

Key environment variables:

| Variable | Description | Default |
|----------|-------------|---------|
| `DATABASE_URL` | PostgreSQL connection string | `postgresql+asyncpg://...` |
| `REDIS_HOST` | Redis host | `localhost` |
| `QDRANT_URL` | Qdrant endpoint | `http://localhost:6333` |
| `DEFAULT_MODEL_ID` | Default LLM model | `mistralai/Mistral-7B-Instruct-v0.2` |
| `EMBEDDING_MODEL` | Embedding model | `sentence-transformers/all-MiniLM-L6-v2` |
| `CACHE_ENABLED` | Enable semantic caching | `true` |
| `RAG_TOP_K` | Number of RAG results | `5` |

See `.env.example` for full configuration options.

## ğŸ¯ Algorithms

### Max-Min Fairness

Maximizes the minimum utility across all agents:

```
maximize   t
subject to:
  - For each agent i: Î£(x_ij Ã— u_ij) â‰¥ t
  - For each good j: Î£(x_ij) â‰¤ 1
  - Budget: Î£(x_ij Ã— v_j) â‰¤ B
  - x_ij â‰¥ 0
```

### Nash Social Welfare

Maximizes the product of agent utilities weighted by entitlements:

```
maximize   Î (U_i ^ w_i)
subject to:
  - For each good j: Î£(x_ij) â‰¤ 1
  - Budget: Î£(x_ij Ã— v_j) â‰¤ B
  - x_ij â‰¥ 0
```

Where:
- `x_ij`: allocation of good j to agent i
- `u_ij`: utility of good j for agent i
- `w_i`: entitlement weight of agent i
- `v_j`: value of good j
- `B`: total budget

## ğŸ›£ï¸ Roadmap

- [ ] Authentication & authorization (JWT)
- [ ] Multi-tenancy support
- [ ] React/TypeScript frontend
- [ ] Streaming chat responses in UI
- [ ] Enhanced document parsing (PDF, DOCX)
- [ ] Graph-based dispute visualization
- [ ] Explanation generation for allocations
- [ ] Integration with external knowledge bases

## ğŸ¤ Contributing

Contributions welcome! Please open an issue or PR.

## ğŸ“„ License

[Specify your license here]

## ğŸ™ Acknowledgments

Built with:
- FastAPI
- HuggingFace Transformers
- Qdrant
- SciPy
- SQLAlchemy
- Redis
- Celery

---

**Note**: This is a framework designed for extensibility. All LLM and embedding models are open-source and can be swapped based on your requirements.
